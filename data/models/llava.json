{
  "model": "llava",
  "url": "https://ollama.com/library/llava",
  "description": "ðŸŒ‹ LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
  "variantsCount": 4,
  "variants": [
    {
      "name": "llava:latest",
      "size": "4.7GB",
      "context": "32K",
      "inputType": "Text, Image"
    },
    {
      "name": "llava:7b",
      "size": "4.7GB",
      "context": "32K",
      "inputType": "Text, Image"
    },
    {
      "name": "llava:13b",
      "size": "8.0GB",
      "context": "4K",
      "inputType": "Text, Image"
    },
    {
      "name": "llava:34b",
      "size": "20GB",
      "context": "4K",
      "inputType": "Text"
    }
  ]
}