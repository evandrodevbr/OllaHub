## ğŸ“‹ AnÃ¡lise
**SolicitaÃ§Ã£o**: Corrigir falhas de conexÃ£o do browser headless e o erro de compilaÃ§Ã£o `Send`, tornar o scraper resiliente com retry e ampliar resultados de busca.
**Objetivo real**: Garantir estabilidade contÃ­nua do backend de scraping, mesmo quando o Chrome headless cai, evitando violaÃ§Ãµes de `Send` e melhorando a cobertura de fontes.

## ğŸ” Contexto Identificado
### Arquivos Examinados
- `src-tauri/src/lib.rs:1176â€“1193` get_or_create_browser: cria/reutiliza `Browser` singleton.
- `src-tauri/src/lib.rs:1247â€“1277` search_web_metadata: comando Tauri que exige `Future + Send`.
- `src-tauri/src/web_scraper.rs:136â€“240` search_duckduckgo_metadata: parsing HTML, previously segurava `Html` atravessando `await`.
- `src-tauri/src/web_scraper.rs:469â€“538` search_and_scrape_with_config: coordena scraping paralelo e coleta resultados.
- `src-tauri/src/web_scraper.rs:605â€“736` fetch_and_convert_sync: cria aba, navega, extrai HTML e usa Readability.
- `src-tauri/src/lib.rs:1306â€“1399` force_kill_browser: encerra processos Chrome headless.

### PadrÃµes Detectados
- **Singleton Browser** via `State<BrowserState>` com `Arc<Browser>`.
- **ConcorrÃªncia controlada** com `Semaphore` e `tokio::spawn_blocking` para operaÃ§Ãµes de Chrome.
- **Fallbacks**: limpeza de URL, filtros anti-ads, Readability + fallback por parÃ¡grafos.

### DependÃªncias Mapeadas
```
web_scraper (modificado)
  â”œâ”€ headless_chrome::Browser/Tab
  â”œâ”€ reqwest (HTTP)
  â”œâ”€ scraper (Html, Selector) [nÃ£o Send]
  â””â”€ readability, html2text
lib (modificado)
  â””â”€ tauri commands (Future + Send obrigatÃ³rio)
```

## ğŸ¯ Abordagens ViÃ¡veis

### Abordagem A: VerificaÃ§Ã£o ativa de Browser + Retry cooperativo
**Conceito**: Validar liveness do `Browser` antes de uso; se cair durante scraping, recriar e repetir uma passada de URLs com limite de tentativas.
```
request â†’ get_or_create_browser (is_alive?) â†’ spawn scraping
  â†³ erro conn fechada â†’ reset + recreate â†’ retry restante (1x)
```
**Vantagens**: âœ“ Simples de integrar, âœ“ Minimiza impacto, âœ“ Boa resiliÃªncia
**Desvantagens**: âœ— Retry global pode repetir algumas URLs
**Complexidade**: O(n) | Tempo: 3â€“4h | Risco: MÃ©dio

### Abordagem B: Pool/Auto-heal por task
**Conceito**: Cada task recupera o browser ao falhar e re-executa sÃ³ sua URL.
```
spawn_blocking(url) â†’ new_tab â†’ erro conn â†’ sinaliza reset â†’ reobtem browser â†’ reprocessa url
```
**Vantagens**: âœ“ Isolamento por URL
**Desvantagens**: âœ— CoordenaÃ§Ã£o de estado entre threads, âœ— Maior complexidade
**Complexidade**: O(n) | Tempo: 5â€“6h | Risco: MÃ©dio-Alto

## ğŸ† RecomendaÃ§Ã£o
**Abordagem A** porque:
- Alinha com o padrÃ£o singleton jÃ¡ existente.
- Menor acoplamento entre tarefas e estado global.
- ImplementaÃ§Ã£o direta com menor risco de condiÃ§Ãµes de corrida.

## âš ï¸ Riscos & MitigaÃ§Ãµes
| Risco | Prob | Impacto | MitigaÃ§Ã£o |
|-------|------|---------|-----------|
| Loop de retry infinito | Baixa | MÃ©dio | Limitar a 1â€“2 tentativas, backoff curto |
| RecriaÃ§Ã£o concorrente do Browser | MÃ©dia | MÃ©dio | Guardar lock no `BrowserState` e checar antes de recriar |
| Demasiadas abas simultÃ¢neas | MÃ©dia | MÃ©dio | Reduzir `max_concurrent_tabs` ao detectar falha |
| DDG bloqueios 429 | MÃ©dia | Baixo | RotaÃ§Ã£o User-Agent e backoff |

## ğŸ“ Plano de ExecuÃ§Ã£o
1. Ajustes `Send` (consolidaÃ§Ã£o)
   - Isolar parsing `scraper::Html` em escopo sÃ­ncrono que retorna `Vec<SearchResultMetadata>` (jÃ¡ aplicado em parte na funÃ§Ã£o; revisar para todos os caminhos).
   - Garantir que nenhum `Html`/`Selector` vive alÃ©m do `await` nos mÃ©todos `search_*`.
   - Verificar pontos: `src-tauri/src/web_scraper.rs:136â€“240` e demais que usem `Html`.

2. Browser Resurrection
   - Adicionar `fn is_browser_alive(browser: &Browser) -> bool` em `web_scraper.rs` ou `lib.rs` (ex.: tentar `new_tab()` e descartar; ou `get_tabs()`; se falhar â†’ false).
   - Alterar `get_or_create_browser` (`src-tauri/src/lib.rs:1176â€“1193`) para: se `Some(browser)` e `!is_alive(browser)` â†’ `reset` e `create_browser()`.
   - Em `force_kill_browser` (`src-tauri/src/lib.rs:1306â€“1399`): apÃ³s kill, setar `BrowserState` para `None` via lock (ou orientar UI a chamar `reset_browser` antes do kill), garantindo recriaÃ§Ã£o no prÃ³ximo uso.

3. Retry resiliente no Scraper
   - Envolver `fetch_and_convert_sync` com camada de retry (mÃ¡x. 2 tentativas) na orquestraÃ§Ã£o async (`search_and_scrape_with_config` em `src-tauri/src/web_scraper.rs:469â€“538`).
   - Se erro contiver "underlying connection is closed"/timeout global do browser:
     - Executar `reset_browser` e `get_or_create_browser` para novo `Arc<Browser)`.
     - Reprocessar as URLs que falharam apenas uma vez com o novo browser.
   - Reduzir temporariamente `max_concurrent_tabs` para 3 ao detectar queda, evitando sobrecarga imediata.

4. PaginaÃ§Ã£o no DuckDuckGo
   - Em `search_duckduckgo` (`src-tauri/src/web_scraper.rs:78â€“133`): suportar paginaÃ§Ã£o via parÃ¢metro `s=<offset>` atÃ© atingir `limit` (30â€“50).
   - Extrair links por pÃ¡gina, deduplicar, respeitar `excluded_domains`/ads.

5. Telemetria e Logs
   - Logar eventos de recreaÃ§Ã£o de browser, tentativas de retry e motivos.
   - Contadores: quedas de browser, tentativas, tempo mÃ©dio por URL.

6. Testes/ValidaÃ§Ã£o
   - Caso 1: ForÃ§ar `force_kill_browser` durante scraping â†’ validar que a prÃ³xima chamada recria o browser e conclui.
   - Caso 2: Simular DDG com >= 40 resultados â†’ paginar e coletar sem violar `Send`.
   - Caso 3: Stress com `max_concurrent_tabs=5` â†’ se cair, reduzir e recuperar.

## Key Changes (Resumo de ImplementaÃ§Ã£o)
- `lib.rs:1176â€“1193`: `get_or_create_browser` verifica liveness; recria se morto.
- `lib.rs:1306â€“1399`: sincronia com `reset_browser` ao matar processos.
- `web_scraper.rs:469â€“538`: retry global (1x) e recriaÃ§Ã£o de browser ao detectar conexÃ£o fechada; ajuste dinamicamente `Semaphore` na segunda passada.
- `web_scraper.rs:78â€“133`: paginaÃ§Ã£o DDG com `s=` offset.
- `web_scraper.rs:136â€“240`: parsing isolado; garantir que `Html` Ã© dropado antes do `await`.

## EntregÃ¡veis
- CÃ³digo robusto em `web_scraper.rs` e `lib.rs` com:
  - CorreÃ§Ã£o total do `Send`.
  - ResurreiÃ§Ã£o automÃ¡tica do browser.
  - Retry controlado por erro de conexÃ£o.
  - PaginaÃ§Ã£o para 30â€“50 resultados.
- Logs melhorados e validaÃ§Ã£o manual automÃ¡tica.

## â“ Aguardando AprovaÃ§Ã£o
- [ ] Posso prosseguir com as mudanÃ§as propostas?
- [ ] PreferÃªncias de `max_concurrent_tabs` e `limit` padrÃ£o (sugestÃ£o: 5 e 40)?
- [ ] Deseja manter `force_kill_browser` como utilitÃ¡rio avanÃ§ado, com alerta na UI para reiniciar o scraper? 